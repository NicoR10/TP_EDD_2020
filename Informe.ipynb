{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFORME\n",
    "\n",
    "En el contexto de la matería Estructuras de datos de la carrera Ingeniería de Computación, se presenta el trabajo integrador de la misma.\n",
    "El trabajo consiste en un programa diseñado en Python que compara los precios de productos de distintas páginas y los devuelve en un archivo de formato de salida específico.\n",
    "\n",
    "La estructura del informe esta organizado en análisis de las páginas selecionadas, diagramas de clase y deciciones de diseño. \n",
    "\n",
    "Para poder ejectuar el programa, es necesario que el usuario tenga instalada ciertas librerias, que se detallan a continuación:\n",
    "- SCRAPY\n",
    "- NLTK (se debe descargar dos paquetes que el programa te indica)\n",
    "- JSONPICKLE\n",
    "\n",
    "Se destaca que el programa fue testeado en distintos sistemas operativos:\n",
    "- Windows 10\n",
    "- Linux Ubuntu 20.04 LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAGRAMAS DE CLASE\n",
    "\n",
    "En la siguiente imagen se puede obser el diagrama de clases del proyecto\n",
    "\n",
    "<img src=\"uml.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECISIONES DE DISEÑO\n",
    "\n",
    "A continuación se explica las decisiones de diseño que se tomaron en cuenta para el trabajo.\n",
    "En primer lugar, al elegir las páginas para analizar, se decidió scrapear toda la página entera, por categorías de productos. Por esto nos referimos a que al ejecutar el programa, el mismo va página por página analizando cada una de las categorías y cada uno de los productos. Esto fue porque no todas las páginas tenian URL con Jquery para realizar la búsqeda de productos y por otro lado, buscamos que todas las páginas tengan una misma estructura para no generar técnicas de scrapeo distintas.\n",
    "\n",
    "Con respecto al menú, esta diseñado para que el usuario avance por las distintas opciones ingresando números que hacen referencia a las distintas opciones. Se aclara que al ejecutar el programa, si el usuario ingresa el número \"1\", el programa llega a su fin.\n",
    "\n",
    "En los spiders, se utilizó la clase Items con el objetivo de organizar el formato de salida de los elementos de buscados.\n",
    "\n",
    "Se decidió que la información de salida (los archivos que se pueden obtener Json, HTML, CSV) se almacenan en una carpeta del proyecto que se llama \"Resultados\". Se toma el path desde la dirección donde se está corriendo el programa y en caso de que no esté creada la carpeta \"Resultados\", la crea.\n",
    "\n",
    "Como archivo de configuración, se creó el archivo \"config.py\". El mismo se detallan las páginas, formatos adminitidos de salida, tipos de busqueda, las start URL y los métodos get de cada uno de los tipos.\n",
    "\n",
    "Se decidió utilizar CrawlerProcess para poder corres los spider. Con esta herramienta se logró poder ejecutar cada uno de los spiders en paralelo. Durante el desarrollo se intentó implementar Reactor pero al utilizarlo se encontraron inconvenientes durante la ejecución y se optó por otra opción.\n",
    "\n",
    "Para los items opcionales, se tuvieron dos consideraciones:\n",
    "- Por un lado, en caso de que los productos no tuviesen precio, se decidió que el precio del producto va a ser igual a cero.\n",
    "- Por otro lado, en cuanto al item que solicita ordenar los productos de una busqueda que ingrese el usuario, nos encontramos con el problema de que un mismo producto, en distintas páginas puede tener un nombre distinto. Ejemplo:\n",
    "    - **Celular Libre Samsung Galaxy S20 Ultra Gris** (Fravega)\n",
    "    - **Celular Libre Samsung S20 6.2\" 8/128 Gris** (Casa Del Audio)\n",
    "    - **CELULAR LIBRE SAMSUNG S20 ULTRA GRIS** (Cetrogar)\n",
    "\n",
    "Para poder solucionar esto, se implementó la librería difflib. Esta librería analizar el porcentaje de exactitud entre distintas frases. Es necesario setear un escalar que me define el nivel de exactitud que quiero obtener. Cero significa que no se quiere exactitud, 1 significa que la búsqueda tiene que ser completamente exacta. Optamos tomar por un valor de 0.8 en base a pruebas empíricas. Se probó tomar unn valor de 1 pero ninguna de las páginas tiene el mismo nombre en cada producto.\n",
    "\n",
    "Para las página de Musimundo, nos encontramos que al buscar los precios de los productos, la página me devuelve dos precios distintos dependiendo de donde lo busque, como se ve en la imagen\n",
    "<img src=\"musimundo_precios.png\">\n",
    "\n",
    "\n",
    "Decidimos en este caso, quedarnos con el menor precio que era del mismo lugar donde nos traemos los precios de los demás productos.\n",
    "\n",
    "Por último, se encontró que a lo largo del código se repetían varías secciones en la lógica utilizada por los spider para definir según los tipos de búsqueda si es válido el producto encontrado. Para eso, se implemento una clase \"utils.py\".\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANÁLISIS DE LAS PÁGINAS\n",
    "\n",
    "Se decidió trabajar con páginas de de electrodomésticos que funcionen a nivel nacional. El primer listado de páginas que se pensó fue el siguiente:\n",
    "- [Frávega](https://www.fravega.com/)\n",
    "- [Musimundo](https://www.musimundo.com/)\n",
    "- [Cetrogar](https://www.cetrogar.com.ar/)\n",
    "- [Rodo](https://www.rodo.com.ar/)\n",
    "- [Casa Del Audio](https://www.casadelaudio.com/)\n",
    "- [Carrefour](https://www.carrefour.com.ar/)\n",
    "- [Falabella](https://www.falabella.com.ar/falabella-ar/)\n",
    "- [Garbarino](https://www.garbarino.com)\n",
    "- [Grupo Marquez](https://grupomarquez.com.ar/)\n",
    "\n",
    "\n",
    "En este punto, lo primero que se analizó fue si cada unas de estas página podía ser scrapeada. \n",
    "Para eso, se ejecutaron los siguiente comandos por consola para analizar lo antes mencionado\n",
    "    \n",
    "    scrapy shell \"URL\"\n",
    "    \n",
    "En los casos de páginas que nos retorna por consola la siguiente linea\n",
    "   [scrapy.core.engine] DEBUG: Crawled (200) <GET URL>\n",
    "    \n",
    "interpretamos que vamos a poder scrapear la página y traernos la información que deseamos.\n",
    "Se encontró dos inconveniente a la hora de seleccionar las páginas. \n",
    "Por un lado, por ejemplo en la página de Rodo y Garbarino, al ejecutar el siguiente comando\n",
    "    scrapy shell \"www.rodo.com.ar\"\n",
    "Se obtuvo lo siguiente\n",
    "[scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://rodo.com.ar/> from <GET https://www.rodo.com.ar/productos.html>\n",
    "\n",
    "Esto nos indica que la página no puede ser scrapeada.\n",
    "Otra página con la que tuvimos problema fue garbarino, que al querer analizar si se puede scrapear nos devolvió la siguientes lineas\n",
    "    [scrapy.core.engine] DEBUG: Crawled (403) <GET https://www.garbarino.com:443/> (referer: None)\n",
    "    [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.garbarino.com:443/> from <GET http://www.garbarino.com>\n",
    "    \n",
    "Por otro lado, ciertas páginas estan realizadas con frameworks relativamente nuevos (EJ: React), que hace que la página sea dinámica y no se pueda acceder a todos los productos de una vez. En esos casos, se descartaron las páginas.\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las páginas con las que finalmente analizamos fue:\n",
    "    - [Frávega](https://www.fravega.com/)\n",
    "    - [Cetrogar](https://www.cetrogar.com.ar/)\n",
    "    - [Casa Del Audio](https://www.casadelaudio.com/)\n",
    "    - [Musimundo](https://www.musimundo.com/)\n",
    "  \n",
    "Como aclaramos anteriormente, lo que hicimos fue buscar páginas que se recorrieran de la misma manera. Es decir, páginas que tuvieran un elemento que tuviera cada una de las categorías que ofrecian y de cada categoría recorer cada uno de los productos.\n",
    "En la siguiente imagen se puede observar un ejemplo con cetrogar\n",
    "\n",
    "\n",
    "<img src=\"cetrogar_categorias.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recorrido de la página se analizó utilizando xpath y buscanod los elementos que van desde el inicio de la página hasta el lsitado de productos. En la imagen que sigue se puede ver un ejemplo\n",
    "\n",
    "\n",
    "<img src=\"cetrogar_xpath.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, el inconveniente que se presentaba era como acceder a los productos que se encuentren en otras páginas.\n",
    "En todas las que utilizamos, encontramos que tiene un boton de \"next\" el cual está asociado a un link mediante la propiedad href. \n",
    "\n",
    "De cada uno de los productos lo que nos trajimos fue el precio, el nombre completo, la categoría del producto y la fecha en la que se busca el producto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonpickle\n",
      "  Using cached jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from jsonpickle) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from importlib-metadata->jsonpickle) (3.1.0)\n",
      "Installing collected packages: jsonpickle\n",
      "Successfully installed jsonpickle-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jsonpickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from menu import Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = Menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Ingresá el producto que deseás buscar o el número 1 para salir.\n",
      "Ingrese el término de búsqueda >> heladera\n",
      "\n",
      "Elija la/s páginas de búsqueda. Si son varias, separadas por coma: (Ej. 1,2,3) \n",
      "\n",
      "1. Cetrogar \n",
      "\n",
      "2. Musimundo \n",
      "\n",
      "3. Frávega \n",
      "\n",
      "4. La Casa Del Audio \n",
      "\n",
      "5. Todas \n",
      "\n",
      "0. Cancelar y salir \n",
      "\n",
      ">> 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:18:39 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-11-16 23:18:39 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Windows-10-10.0.19041-SP0\n",
      "2020-11-16 23:18:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2020-11-16 23:18:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2020-11-16 23:18:39 [scrapy.extensions.telnet] INFO: Telnet Password: fbacd5071eb6421d\n",
      "2020-11-16 23:18:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando: heladera\n",
      "Tipo: Alguna de las palabras\n",
      "En: \n",
      "\n",
      "Frávega\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:18:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-16 23:18:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-16 23:18:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-16 23:18:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-16 23:18:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-16 23:18:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-16 23:18:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/> (referer: None)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/tv-audio-video> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.fravega.com/l/tv-y-video/tv/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/mas-categorias> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/hogar-muebles-jardin> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/electrodomesticos-aires-ac> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/bebes-ninos> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/camaras-y-video-camaras/> (referer: https://www.fravega.com/categorias/tv-audio-video)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/tecnologia-celulares> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/audio/> (referer: https://www.fravega.com/categorias/tv-audio-video)\n",
      "2020-11-16 23:18:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/tv-y-video/tv/> (referer: https://www.fravega.com/categorias/tv-audio-video)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/categorias/salud-belleza-fitness> (referer: https://www.fravega.com/)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/camping-y-aire-libre/> (referer: https://www.fravega.com/categorias/mas-categorias)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/jardin/piletas-y-piscinas/> (referer: https://www.fravega.com/categorias/mas-categorias)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/tv-y-video/accesorios/> (referer: https://www.fravega.com/categorias/tv-audio-video)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/herramientas-y-construccion/herramientas/herramientas-de-jardin/> (referer: https://www.fravega.com/categorias/mas-categorias)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/moda> (referer: https://www.fravega.com/categorias/mas-categorias)\n",
      "2020-11-16 23:18:42 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.fravega.com/l/seguridad-para-el-hogar/> from <GET https://www.fravega.com/seguridad-para-el-hogar/>\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/hogar/bazar/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/proyectores-y-pantallas/> (referer: https://www.fravega.com/categorias/tv-audio-video)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/jardin/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/pequenos-electrodomesticos/cocina/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/juguetes-y-juegos/> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=autos-motos-y-otros/accesorios-para-autos-y-motos,equipos-para-auto> (referer: https://www.fravega.com/categorias/mas-categorias)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/impresoras/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=bebes-y-primera-infancia/butacas-auto-y-huevitos> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/deportes-y-fitness/> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/accesorios-de-informatica/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=equipos-para-auto> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/belleza-y-cuidado-corporal/> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/salud-y-belleza/perfumes/> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/salud-y-bienestar/> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/seguridad-para-el-hogar/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/pequenos-electrodomesticos/cuidado-personal/> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/pequenos-electrodomesticos/hogar/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/celulares/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/relojes> (referer: https://www.fravega.com/categorias/salud-belleza-fitness)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/celulares/celulares-liberados/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/videojuegos/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=bebes-y-primera-infancia> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/gaming-pc/> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/bebes-y-primera-infancia/seguridad/> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/informatica/almacenamiento> (referer: https://www.fravega.com/categorias/tecnologia-celulares)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/bebes-y-primera-infancia/cochecitos/> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/climatizacion/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/termotanques-y-calefones/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/lavado/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=bebes-y-primera-infancia/alimentacion-y-lactancia> (referer: https://www.fravega.com/categorias/bebes-ninos)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/muebles/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/heladeras-freezers-y-cavas/> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-gafa-hgf357afb-blanca-286lts-161088',\n",
      " 'market': 'fravega',\n",
      " 'price': 41499.0,\n",
      " 'title': 'Heladera Cíclica Gafa HGF357AFB Blanca 286Lts'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-sigma-2f1200b-239lt-160681',\n",
      " 'market': 'fravega',\n",
      " 'price': 35999.0,\n",
      " 'title': 'Heladera Cíclica Sigma 2F1200B 239lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-patrick-hpk135m00b01-277lt-160763',\n",
      " 'market': 'fravega',\n",
      " 'price': 40499.0,\n",
      " 'title': 'Heladera Cíclica Patrick HPK135M00B01 277Lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-peabody-2f1200p-239lt-160643',\n",
      " 'market': 'fravega',\n",
      " 'price': 39999.0,\n",
      " 'title': 'Heladera Cíclica Peabody 2F1200P 239lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-gafa-hgf387afb-blanca-375lts-160979',\n",
      " 'market': 'fravega',\n",
      " 'price': 53999.0,\n",
      " 'title': 'Heladera Cíclica Gafa HGF387AFB Blanca 375Lts'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-con-freezer-gafa-hgf377afb-326-lt-blanca-50003305',\n",
      " 'market': 'fravega',\n",
      " 'price': 44699.0,\n",
      " 'title': 'Heladera con freezer Gafa HGF377AFB 326 Lt  Blanca'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-gafa-hgf357afp-plata-286lts-160694',\n",
      " 'market': 'fravega',\n",
      " 'price': 44999.0,\n",
      " 'title': 'Heladera Cíclica Gafa HGF357AFP Plata 286Lts'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-peabody-2f1200b-239lt-160674',\n",
      " 'market': 'fravega',\n",
      " 'price': 35999.0,\n",
      " 'title': 'Heladera Cíclica Peabody 2F1200B 239lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-no-frost-peabody-nf1600sd-329lt-160676',\n",
      " 'market': 'fravega',\n",
      " 'price': 69999.0,\n",
      " 'title': 'Heladera No Frost Peabody NF1600SD 329lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-no-frost-peabody-nf1600p-329lt-160655',\n",
      " 'market': 'fravega',\n",
      " 'price': 64999.0,\n",
      " 'title': 'Heladera No Frost Peabody NF1600P 329lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-gafa-hgf387afp-plata-375lts-160865',\n",
      " 'market': 'fravega',\n",
      " 'price': 57999.0,\n",
      " 'title': 'Heladera Cíclica Gafa HGF387AFP Plata 375Lts'}\n",
      "2020-11-16 23:18:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.fravega.com/l/heladeras-freezers-y-cavas/>\n",
      "{'categoria': 'Heladeras, Freezers y Cavas',\n",
      " 'fecha': '16/11/2020 23:18:45',\n",
      " 'link': 'https://www.fravega.com/p/heladera-ciclica-patrick-hpk151m00b-394lt-160504',\n",
      " 'market': 'fravega',\n",
      " 'price': 51999.0,\n",
      " 'title': 'Heladera Cíclica Patrick HPK151M00B 394Lt'}\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/colchones-y-sommiers/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/?categorias=cocina/cocinas,cocina/hornos> (referer: https://www.fravega.com/categorias/electrodomesticos-aires-ac)\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/iluminacion/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/hogar/decoracion/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.fravega.com/l/hogar/bano/> (referer: https://www.fravega.com/categorias/hogar-muebles-jardin)\n",
      "2020-11-16 23:18:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-16 23:18:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 19501,\n",
      " 'downloader/request_count': 55,\n",
      " 'downloader/request_method_count/GET': 55,\n",
      " 'downloader/response_bytes': 2801008,\n",
      " 'downloader/response_count': 55,\n",
      " 'downloader/response_status_count/200': 54,\n",
      " 'downloader/response_status_count/301': 1,\n",
      " 'dupefilter/filtered': 4,\n",
      " 'elapsed_time_seconds': 5.810882,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 17, 2, 18, 45, 705209),\n",
      " 'item_scraped_count': 12,\n",
      " 'log_count/DEBUG': 68,\n",
      " 'log_count/INFO': 10,\n",
      " 'request_depth_max': 2,\n",
      " 'response_received_count': 54,\n",
      " 'scheduler/dequeued': 55,\n",
      " 'scheduler/dequeued/memory': 55,\n",
      " 'scheduler/enqueued': 55,\n",
      " 'scheduler/enqueued/memory': 55,\n",
      " 'start_time': datetime.datetime(2020, 11, 17, 2, 18, 39, 894327)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-16 23:18:45 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Desea exportar los resultados? [y/n] >> y\n",
      "Elija el formato: \n",
      "\n",
      "1. csv \n",
      "\n",
      "2. json \n",
      "\n",
      "3. html \n",
      "\n",
      "0. Cancelar y salir \n",
      "\n",
      ">> 2\n",
      "Exportando: ...\n",
      "Tipo: json\n",
      "\n",
      "¿Desea obtener una estadística o tabla comparativa?. \n",
      "\n",
      "1. Estadísticas por modelo \n",
      "\n",
      "2. Estadísticas por promedio \n",
      "\n",
      "3. Unicos por tienda \n",
      "\n",
      "0. No. Finalizar. \n",
      "\n",
      ">> 1\n",
      "Fin del programa.\n"
     ]
    }
   ],
   "source": [
    "menu.ejecutar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Los archivos han sido creados en una carpeta llamada \"Resultados\" en el path donde se esta ejecutando este documento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (5.4.3)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (1.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (20.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (8.4.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (0.2.5)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest) (1.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: colorama in c:\\users\\eduardo\\appdata\\roaming\\python\\python38\\site-packages (from pytest) (0.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from packaging->pytest) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\eduardo\\appdata\\roaming\\python\\python38\\site-packages (from packaging->pytest) (1.15.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipytestNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading ipytest-0.9.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: ipython in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipytest) (7.16.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipytest) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: pytest>=5.4 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipytest) (5.4.3)\n",
      "Requirement already satisfied, skipping upgrade: pygments in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (49.2.0.post20200714)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: backcall in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in c:\\users\\eduardo\\appdata\\roaming\\python\\python38\\site-packages (from ipython->ipytest) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from ipython->ipytest) (0.17.1)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\eduardo\\appdata\\roaming\\python\\python38\\site-packages (from packaging->ipytest) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from packaging->ipytest) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (8.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from pytest>=5.4->ipytest) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from traitlets>=4.2->ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: parso<0.8.0,>=0.7.0 in c:\\users\\eduardo\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: ipytest\n",
      "Successfully installed ipytest-0.9.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install -U ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................                                                   [100%]\n",
      "22 passed in 0.08s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from utils import Utils\n",
    "from procesador import Procesador\n",
    "from os import path\n",
    "import pathlib\n",
    "import datetime\n",
    "from config import Config\n",
    "import menu\n",
    "from menu import QuiereSalirException\n",
    "import unittest\n",
    "from unittest.mock import patch\n",
    "\n",
    "utils = Utils()\n",
    "def test_tipo_busqueda_1():\n",
    "    '''Publicación con la frase exacta'''\n",
    "    target = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    title = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    assert utils.tipo_busqueda_1(target, title)\n",
    "\n",
    "def test_tipo_busqueda_2():\n",
    "    '''Publicación que contenga todas las palabras'''\n",
    "    target = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    title = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    assert utils.tipo_busqueda_2(target, title)\n",
    "\n",
    "def test_tipo_busqueda_2_v1():\n",
    "    '''Publicación que contenga todas las palabras'''\n",
    "    target = 'Lavarropas CARGA Frontal'\n",
    "    title = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    assert utils.tipo_busqueda_2(target, title)\n",
    "\n",
    "def test_tipo_busqueda_3():\n",
    "    '''Publicación que contenga algunas de las palabras.'''\n",
    "    target = 'Lavarropas Longvie'\n",
    "    title = 'Lavarropas Carga Frontal Longvie 8 Kg 1200 RPM L18012'\n",
    "    assert utils.tipo_busqueda_3(target, title)\n",
    "\n",
    "def test_tipo_busqueda_3_v1():\n",
    "    '''Publicación que contenga todas las palabras'''\n",
    "    target = 'Celular Samsung'\n",
    "    title = 'CELULAR LIBRE SAMSUNG S20 ULTRA GRIS'\n",
    "    assert utils.tipo_busqueda_3(target, title)\n",
    "\n",
    "#fragmeto extraido de un scrapeo donde criterio fue longvie, pero para lo cambio a restados_test para test\n",
    "lista = [{'categoria': 'Electrodomésticos','fecha': '15/11/2020 12:26:02',\n",
    " 'link': 'https://www.cetrogar.com.ar/aspiradora-electrolux-gt30n-1300w.html',\n",
    " 'market': 'cetrogar', 'price': 12109.0,\n",
    " 'title': 'Calefactor Longvie EC3200ECA3 Con Multigas'}, {'categoria': 'Electrodomésticos',\n",
    " 'fecha': '15/11/2020 12:26:07',\n",
    " 'link': 'https://www.cetrogar.com.ar/aspiradora-yelmo-as-3228-2000w-sin-bolsa.html',\n",
    " 'market': 'cetrogar', 'price': 23899.0,\n",
    " 'title': 'Termotanque Longvie Electrico para colgar TE40F Carga Inferior'}, {'categoria': 'Electrodomésticos',\n",
    " 'fecha': '15/11/2020 12:26:07',\n",
    " 'link': 'https://www.cetrogar.com.ar/cafetera-nespressor-inissia-red-aeroccino-3-a3c40-ar-re-ne.html',\n",
    " 'market': 'cetrogar', 'price': 29999.0,\n",
    " 'title': 'Termotanque Longvie Multigas Colgar 50 Litros inferior'}, {'categoria': 'Electrodomésticos',\n",
    " 'fecha': '15/11/2020 12:26:08',\n",
    " 'link': 'https://www.cetrogar.com.ar/microondas-bgh-qchef-28l-b228db-bl.html',\n",
    " 'market': 'cetrogar', 'price': 43799.0,\n",
    " 'title': 'Lavarropas Longvie L16508 6.5Kg 800rpm'}, {'categoria': 'Electrodomésticos',\n",
    " 'fecha': '15/11/2020 12:26:08',\n",
    " 'link': 'https://www.cetrogar.com.ar/anafe-longvie-a-6600g-x-4-hornallas-gas.html',\n",
    " 'market': 'cetrogar', 'price': 48999.0, 'title': 'Anafe Longvie A-6600G x 4 Hornallas Gas'}]\n",
    "\n",
    "criterio = 'resultados test'.replace(' ','_')\n",
    "procesador = Procesador(lista, criterio)\n",
    "\n",
    "def test_imprimir_archivo_csv():\n",
    "    procesador.imprimirArchivo('csv')\n",
    "    filename = criterio + \"_\" + datetime.datetime.now().strftime(\"%d-%m-%Y\") + '.csv'\n",
    "    assert path.isfile('resultados/' + filename)\n",
    "\n",
    "def test_imprimir_archivo_json():\n",
    "    procesador.imprimirArchivo('json')\n",
    "    filename = criterio + \"_\" + datetime.datetime.now().strftime(\"%d-%m-%Y\") + '.json'\n",
    "    assert path.isfile('resultados/' + filename)\n",
    "\n",
    "def test_imprimir_archivo_html():\n",
    "    procesador.imprimirArchivo('html')\n",
    "    filename = criterio + \"_\" + datetime.datetime.now().strftime(\"%d-%m-%Y\") + '.html'\n",
    "    assert path.isfile('resultados/' + filename)\n",
    "\n",
    "config = Config()\n",
    "def test_config_paginas():\n",
    "    assert config.get_paginas()['5'] == 'Todas'\n",
    "\n",
    "def test_config_formatos_admitidos():\n",
    "    assert config.get_formatos_admitidos()['2'] == 'json'\n",
    "\n",
    "def test_config_tipos():\n",
    "    assert config.get_tipos()['3'] == 'Alguna de las palabras'\n",
    "\n",
    "def test_config_url():\n",
    "    assert config.get_start_url()['fravega'] == 'https://www.fravega.com/'\n",
    "\n",
    "def test_config_extras():\n",
    "    assert config.get_extras()['1'] == 'Estadísticas por modelo'\n",
    "\n",
    "#Test de Menu\n",
    "class TestMenu(unittest.TestCase):\n",
    "    \n",
    "    menu_t = menu.Menu()\n",
    "    \n",
    "    # Test para formato '1'\n",
    "    @patch(\"builtins.input\", return_value=\"1\")\n",
    "    def test_pedir_formato_1(self, mock_input):\n",
    "        self.assertEqual(self.menu_t._Menu__pedir_formato(), \"1\")\n",
    "        \n",
    "    # Test para input '0' (Quiere salir)\n",
    "    @patch(\"builtins.input\", return_value=\"0\")\n",
    "    def test_pedir_formato_0(self, mock_input):\n",
    "        with self.assertRaises(QuiereSalirException):\n",
    "            self.menu_t._Menu__pedir_formato()\n",
    "            \n",
    "    # Test para formato pedir pagina\n",
    "    @patch(\"builtins.input\", return_value=\"3\")\n",
    "    def test_pedir_pagina_3(self, mock_input):\n",
    "        self.assertEqual(self.menu_t._Menu__pedir_pagina(), [\"3\"])\n",
    "        \n",
    "    # Test para formato pedir varias paginas\n",
    "    @patch(\"builtins.input\", return_value=\"3,1,2\")\n",
    "    def test_pedir_paginas(self, mock_input):\n",
    "        self.assertEqual(self.menu_t._Menu__pedir_pagina(), [\"3\", '1', '2'])\n",
    "        \n",
    "    # Test para input '0' (Quiere salir)\n",
    "    @patch(\"builtins.input\", return_value=\"0\")\n",
    "    def test_pedir_pagina_0(self, mock_input):\n",
    "        with self.assertRaises(QuiereSalirException):\n",
    "            self.menu_t._Menu__pedir_pagina()\n",
    "    \n",
    "    # Test para formato pedir tipo busqueda\n",
    "    @patch(\"builtins.input\", return_value=\"2\")\n",
    "    def test_pedir_tipo_busqueda(self, mock_input):\n",
    "        self.assertEqual(self.menu_t._Menu__tipo_busqueda(), '2')\n",
    "        \n",
    "    # Test para input '0' (Quiere salir)\n",
    "    @patch(\"builtins.input\", return_value=\"0\")\n",
    "    def test_pedir_tipo_busqueda_0(self, mock_input):\n",
    "        with self.assertRaises(QuiereSalirException):\n",
    "            self.menu_t._Menu__tipo_busqueda()\n",
    "            \n",
    "    # Test para formato pedir extras\n",
    "    @patch(\"builtins.input\", return_value=\"2\")\n",
    "    def test_pedir_extra_2(self, mock_input):\n",
    "        self.assertEqual(self.menu_t._Menu__pedir_extras(), '2')\n",
    "        \n",
    "    # Test para input '0' (Quiere salir)\n",
    "    @patch(\"builtins.input\", return_value=\"0\")\n",
    "    def test_pedir_extra_0(self, mock_input):\n",
    "        with self.assertRaises(QuiereSalirException):\n",
    "            self.menu_t._Menu__pedir_extras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
